# ğŸ§  RAG Demo with LM Studio & Streamlit

This project is a **Retrieval-Augmented Generation (RAG)** demo using a **locally hosted Mistral-7B-Instruct model** in **LM Studio**, with an interactive front-end built in **Streamlit**.  
It allows users to upload `.pdf` and `.txt` files, ask questions in natural language, and receive accurate, grounded answers based on the uploaded documents.

---

## ğŸ¯ Features

- âœ… Works fully **offline** using **LM Studio**
- ğŸ“„ Supports `.txt` and `.pdf` document uploads
- ğŸ” Smart chunking with sliding window and overlap
- ğŸ§  Dense semantic retrieval using **FAISS** + **SentenceTransformers**
- ğŸ¤– Answers generated by Mistral using only retrieved context
- ğŸ” Displays numbered source files used to generate each answer

---

## ğŸš€ Getting Started

### 1. Install Python dependencies:

```bash
pip install -r requirements.txt
```

### 2.  Run LM Studio:
- Open LM Studio

- Load the model mistralai/mistral-7b-instruct-v0.3 (or any other instruct-compatible model)

- Ensure LM Studio is listening on http://localhost:1234

### 3. start the app

```bash
streamlit run app.py
```
---

Project Structure

rag_demo/
â”œâ”€â”€ app.py                # Streamlit interface

â”œâ”€â”€ rag_engine.py         # Core logic for loading, embedding, retrieval, generation

â”œâ”€â”€ requirements.txt      # Python dependencies

â”œâ”€â”€ docs/                 # Uploaded .txt and .pdf documents

---

Requirements

- Python 3.8+

- LM Studio installed and running locally

- Model downloaded: mistralai/mistral-7b-instruct-v0.3 (or similar)

- Internet access required only for downloading models/dependencies

