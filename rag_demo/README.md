# 🧠 RAG Demo with LM Studio & Streamlit

This project is a **Retrieval-Augmented Generation (RAG)** demo using a **locally hosted Mistral-7B-Instruct model** in **LM Studio**, with an interactive front-end built in **Streamlit**.  
It allows users to upload `.pdf` and `.txt` files, ask questions in natural language, and receive accurate, grounded answers based on the uploaded documents.

---

## 🎯 Features

- ✅ Works fully **offline** using **LM Studio**
- 📄 Supports `.txt` and `.pdf` document uploads
- 🔍 Smart chunking with sliding window and overlap
- 🧠 Dense semantic retrieval using **FAISS** + **SentenceTransformers**
- 🤖 Answers generated by Mistral using only retrieved context
- 🔎 Displays numbered source files used to generate each answer

---

## 🚀 Getting Started

### 1. Install Python dependencies:

```bash
pip install -r requirements.txt
```

### 2.  Run LM Studio:
- Open LM Studio

- Load the model mistralai/mistral-7b-instruct-v0.3 (or any other instruct-compatible model)

- Ensure LM Studio is listening on http://localhost:1234

### 3. start the app

```bash
streamlit run app.py
```
---

Project Structure

rag_demo/
├── app.py                # Streamlit interface

├── rag_engine.py         # Core logic for loading, embedding, retrieval, generation

├── requirements.txt      # Python dependencies

├── docs/                 # Uploaded .txt and .pdf documents

---

Requirements

- Python 3.8+

- LM Studio installed and running locally

- Model downloaded: mistralai/mistral-7b-instruct-v0.3 (or similar)

- Internet access required only for downloading models/dependencies

